{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b38611",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addee9b8",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "<ul>\n",
    "<li><a> Problem Statement  </a></li>\n",
    "<li><a> k - fold Cross Validation </a></li>\n",
    "<li><a> k - fold Cross Validation Algorithm</a></li>   \n",
    "<li><a> Significance of Standardisation</a> </li>\n",
    "\n",
    "<ul>\n",
    "    <li><a>Why is it important?</a></li>\n",
    "    <li><a>How are variables Standardised??</a></li>\n",
    "    </ul>\n",
    "<li><a>Types of Performance Metrics</a></li>\n",
    "<li><a>Modus Operandi</a></li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc19b7f",
   "metadata": {},
   "source": [
    "#### Problem Statement:\n",
    "\n",
    "\n",
    "Create a function for k-fold Cross Validation, $ \\texttt{CV}$ (model,X_train,Y_train,$\\textbf{k}$), which returns Cross validation Score as output by computing Validation Score of k- samples with Mean Squared Error as the Error metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47744abf",
   "metadata": {},
   "source": [
    "#### $\\textbf{k}$ - fold Cross Validation:\n",
    "\n",
    "For a sufficiently large sized sample, the conventional train-validate-test methodology is deployed to fit a model onto the data. However, in cases where sample size is significantly smaller, there is not enough scope for the sample to be sliced into appropriate proportions of train, validate and test data. To address this issue, a different methodology has been formulated where sample is randomly split into $\\textbf{k}$ - smaller samples of identical size and thereby allowing the model to keep little more data for training.\n",
    "\n",
    "The model runs for $\\textbf{k}$ no.of iterations and takes a different ($\\textbf{k-1}$) no.of samples as training data for each iteration to fit the model. The model thus fit is then tested on the left-over sample and the corresponding validation score is computed. The validation scores of all the iterations are then averaged as Arithmetic mean or Weighted average based on heterogenity of sample sizes. The average thus obtained is termed as Cross-validation Score and the technique is called $\\textbf{k}$ - fold Cross Validation.\n",
    "\n",
    "Cross validation is therefore a technique preferred when there is not enough data available for training set. Though not regarded as a thumb rule, it is preferred to have larger $\\textbf{k}$ i.e slicing data into many samples ($ S_{1},S_{2},S_{3}.....S_{k} $) for smaller sized sample $ S $.\n",
    "\n",
    "Smaller the Cross-Validation score, better fit the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813c5bd",
   "metadata": {},
   "source": [
    "#### $\\textbf{k}$ - fold Cross Validation algorithm:\n",
    "\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "1. Randomly divide the sample into $\\textbf{k}$ equal parts $ S_{1},S_{2},S_{3}.....S_{k} $ such that $ S_{1} \\cup S_{2} \\cup S_{3}  \\cup ......\\cup S_{k} = S $(Master Sample)  \n",
    "\n",
    "   \\& $ S_{i} \\cap S_{j} = \\varnothing $ , $\\forall$   $i \\neq j $\n",
    "   \n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>2</td>\n",
    "      <td>3</td>\n",
    "      <td>.......</td>\n",
    "      <td>k-1</td>\n",
    "      <td>k</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "   \n",
    "   \n",
    "2. for i from 1 to k do:\n",
    "\n",
    "\n",
    "3. hold out = $S_{i}$\n",
    "\n",
    "\n",
    "4. training data  = $ \\underset{j\\neq i}{\\underset{j=1}{\\overset{k}{\\bigcup}}} S_{j}$\n",
    "\n",
    "\n",
    "5. Train Model M on training data\n",
    "\n",
    "\n",
    "6. Validation score = Validation score of model M on holdout sample\n",
    "\n",
    "\n",
    "7. end for\n",
    "\n",
    "\n",
    "8. Cross Validation Score = Average of all validation scores.\n",
    "\n",
    "    If samples are of equal size $\\Rightarrow$ Simple Average.\n",
    "    \n",
    "    If samples are of unequal size $\\Rightarrow$ Weighted Average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5355853",
   "metadata": {},
   "source": [
    "\n",
    "$\\textbf{Simple Average}$ = $\\dfrac{V_{1}+V_{2}+V_{3}+.....+V_{k}}{k}$\n",
    "\n",
    "where  $V_{1},V_{2}....V_{k}$ are respective validation scores of k-samples.\n",
    "\n",
    "$\\textbf{Weighted Average}$ = $\\dfrac{n_{1}V_{1}+n_{2}V_{2}+n_{3}V_{3}.....+n_{k}V_{k}}{n_{1}+n_{2}+n_{3}+.....+n_{k}}$\n",
    "\n",
    "where $n_{1},n_{2},n_{3}...$ are sizes of individual samples and $V_{1},V_{2}....V_{k}$ are  respective validation scores of k-samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dddb32",
   "metadata": {},
   "source": [
    "### Significance of Standardisation:\n",
    "\n",
    "\n",
    "#### Why is it important???\n",
    "\n",
    "\n",
    "The variables that are used as predictors are usually quite disparate. This disparate nature acts as a hindrance for the user to compare them across the board as they are spread across wide orders of magnitude. In case of Linear Regression, the coefficients of fitted model cannot be compared as the predictors are of different range. In such a scenario, Standardisation comes to the rescue by bringing variables on an equivalent scale and facilitating comparison between them.\n",
    "\n",
    "\n",
    "#### How are variables Standardised??\n",
    "\n",
    "For a variable X, \n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{Standardization} = \\dfrac{X - \\mu\\left(X\\right)}{std\\left(X\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "After Standardisation, all the predictors will have a common mean of 0 and a standard deviation of 1 thus allowing comparison between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6c467",
   "metadata": {},
   "source": [
    "#### Types of Performance Metrics:\n",
    "\n",
    "* Mean Absolute Deviation\n",
    "\n",
    "* Mean Squared Error\n",
    "\n",
    "* Root Mean Squared Error\n",
    "\n",
    "\n",
    "In this problem, we use Mean Squared Error(MSE) as a performance metric/error metric to compute the validation score on the holdout sample.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{Mean Squared Error (MSE)} = \\dfrac{1}{n} \\underset{i=1} {\\overset{n}{\\sum}} \\left( y_{i}^{test}- {\\hat{y_{i}}}\\right)^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb5110",
   "metadata": {},
   "source": [
    "**Kindly go through the Modus Operandi before executing the function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd69648",
   "metadata": {},
   "source": [
    "#### Modus Operandi:\n",
    "\n",
    "1. Cars dataset is randomly shuffled.\n",
    "\n",
    "\n",
    "2. Categorical variables are dropped from the dataframe to ensure the calculation of aggregate functions mean and standard deviation.\n",
    "\n",
    "\n",
    "3. The dataset is then Standardised using the expression given above.\n",
    "\n",
    "\n",
    "4. From the standardised dataset, X_train and Y_train are sent in as input parameters into the function $ \\texttt{CV}$ (model,X_train,Y_train,$\\textbf{k}$). \n",
    "\n",
    "\n",
    "5. Inside the function, the dataset is sliced into $\\textbf{k}$-parts and a model(OLS Linear Regression in this case) is fit on ($\\textbf{k-1}$) samples by holding out a sample $S_{i}$, where i is in range of 1 to k.\n",
    "\n",
    "\n",
    "6. The model is then fit on the hold out sample $S_{i}$ and the validation score is evaluated.\n",
    "    \n",
    "    #### Note: In this function, Mean Square Error is used as performance metric to calculate Validation score.\n",
    "\n",
    "\n",
    "7. After iterating on all the samples, Cross Validation score is returned as output. The model which returns the least Cross validation score is regarded as the ideal model.\n",
    "\n",
    "\n",
    "*Comments:*\n",
    "\n",
    "**i) The function used in this notebook is confined to Ordinary Least Squares(OLS) Linear Regression model and hence model object is not entered as input parameter to the function.**\n",
    "\n",
    "**ii) The function can fit a Linear Regression Model on one (or) more predictors.**\n",
    "\n",
    "**iii) The sample is split into k-folds and each of these samples are appended into an empty list so that the user can access individual samples.**\n",
    "\n",
    "\n",
    "(X_train and Y_train need not necessarily be Weight and MPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a29d8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required packages\n",
    "import statsmodels.api as stm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05af5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the dataframe data from cars.csv\n",
    "data = pd.read_csv(r\"C:\\Users\\anjit\\Documents\\cars.csv\")\n",
    "\n",
    "#Shuffle the data \n",
    "data = data.sample(len(data))\n",
    "\n",
    "#Drop the nominal columns from the dataframe\n",
    "data = data.drop(['Car','Origin','Model'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "924b2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function CV outputs the cross validation score, pass the predictors as X_train and Target variable as Y_train\n",
    "#and k value (number of groups the data sample is to be split into)\n",
    "\n",
    "def CV(X_train,Y_train,k):\n",
    "    #initialization of arr array, used for calculation of cv score \n",
    "    arr = np.array([])\n",
    "    #initializing an empty list s which stores the samples\n",
    "    s = []\n",
    "    \n",
    "    #Standardize the data\n",
    "    X_train = ((X_train - np.mean(X_train))/np.std(X_train))\n",
    "    \n",
    "    # n stores the number of predictors. When X_train is a series, length of columns returns AttributeError,\n",
    "    # so we convert the series to dataframe and then calculate the value of n\n",
    "    try:\n",
    "        n = len(X_train.columns)\n",
    "    except AttributeError:\n",
    "        #converting series to data frame\n",
    "        X_train = X_train.to_frame()\n",
    "        n = len(X_train.columns)\n",
    "    \n",
    "    Y_train = Y_train.to_frame()\n",
    "    #train stores the join of X_train and Y_train by concatenation along the vertical axis(axis = 1),\n",
    "    #this is done before spliting the data into samples\n",
    "    train = pd.concat([X_train,Y_train],axis=1)\n",
    "    \n",
    "        \n",
    "    #Split the data into k samples\n",
    "    \n",
    "    #Check whether the length of data is divisible by k, if yes, divide the data into k equal parts\n",
    "    if len(data)%k == 0:\n",
    "        for i in range(k):\n",
    "            #d stores the number of data elements in a sample\n",
    "            d = len(data)//k\n",
    "            #train dataframe is divided into samples, each with d elements\n",
    "            sample = train.iloc[d*i : d*(i+1)]\n",
    "            #append each sample to s\n",
    "            s.append(sample)\n",
    "    #If length of data is not exactly divisible by k, create k-1 samples of equal length, \n",
    "    #the remaining data is part of kth sample\n",
    "    else:\n",
    "        for i in range(k-1):\n",
    "            d = len(data)//k\n",
    "            sample= train.iloc[d*i : d*(i+1)]\n",
    "            s.append(sample)\n",
    "        sample = train.iloc[d*(k-1):]\n",
    "        s.append(sample)\n",
    "    \n",
    "    #for loop for cross validation, for k samples\n",
    "    for j in range(k): \n",
    "        #Store jth sample as holdout,validation is done on the holdout sample\n",
    "        holdout = s[j]\n",
    "    \n",
    "        #the rest of the data(other than holdout) is stored as training data\n",
    "        #df1 and df2 stores data from train dataframe excluding the jth sample\n",
    "        df1 = train.iloc[0:(d*j)]\n",
    "        df2 = train.iloc[(d*(j+1)):]\n",
    "    \n",
    "        #train_new is the union of df1 and df2 by concatenation along the horizontal axis(axis = 0)\n",
    "        train_new = pd.concat([df1,df2],axis = 0)   \n",
    "    \n",
    "        #Get all the predictors\n",
    "        X = train_new.loc[:,X_train.columns]\n",
    "            \n",
    "       #Store the target variable \n",
    "        Y = train_new.loc[:,Y_train.columns]\n",
    "        \n",
    "        #Adding constant to X (predictors)\n",
    "        X = stm.add_constant(X)\n",
    "     \n",
    "        #Fitting the line through linear regression\n",
    "        linreg = stm.OLS(Y,X).fit()\n",
    "    \n",
    "        #calculating y_pred\n",
    "        x = 0\n",
    "        #Adding the intercept(b0) to y_pred\n",
    "        y_pred = linreg.params[0]\n",
    "        while x <n:\n",
    "            #adding the coeff*predictor value to y_pred\n",
    "            y_pred += (linreg.params[x+1]*(holdout.iloc[:,x]))\n",
    "            x += 1\n",
    "       #Slicing y_test values from holdout sample\n",
    "        y_test = holdout.iloc[:,(len(holdout.columns)-1)]\n",
    "        \n",
    "        # calculating mean squared error\n",
    "        mse = (np.dot(np.transpose(y_pred-y_test),(y_pred-y_test)))/len(s[j])\n",
    "        weight = len(s[j])*mse\n",
    "        arr = np.append(arr,weight)\n",
    "    #Calculate the cross validation score. As the number of elements is not equal in all samples , \n",
    "    #we calculate weighted avg of validation score\n",
    "    cv = np.sum(arr)/len(train)\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87032e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.940841478931667"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Function call for CV, pass the predictors as X_train and Target variable as Y_train\n",
    "#and k value (number of groups the data sample is to be split into)\n",
    "CV(data['Weight'],data.MPG,150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
